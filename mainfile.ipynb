{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "#---------- Library Imports ----------\n",
                "import torch\n",
                "import cv2\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "from torchvision import transforms\n",
                "import os\n",
                "import sequential_creator\n",
                "import glob\n",
                "from torchmetrics import MeanAbsolutePercentageError as MAPE\n",
                "import matplotlib as mpl\n",
                "import re\n",
                "import json\n",
                "from tqdm import tqdm\n",
                "import pytorch_lightning as pl\n",
                "from torch.utils.data import DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "%reload_ext autoreload"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Personal python files imports ----------\n",
                "import load_lettuce_dataset\n",
                "import data_augmentation\n",
                "import model_and_training_files\n",
                "import lettuce_dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Hyperparameters ----------\n",
                "batch_size = 16\n",
                "learning_rate = 1e-3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Other parameters ----------\n",
                "augmented_dataset_size = 341 # Size of the augmented dataset, so if the original dataset contained 103 images, they would be augmented and made into 2000 images\n",
                "                              # One thing to note is that if this number is much bigger than the size of the original dataset, then they would most likely end up being duplicates, since there is not that many augmentations implemented at the moment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 341/341 [00:00<00:00, 392.35it/s]\n"
                    ]
                }
            ],
            "source": [
                "#---------- Load Lettuce Dataset ----------\n",
                "rgb_list, depth_list, fresh_weight_list, dry_weight_list = load_lettuce_dataset.load_all_images()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Augmenting RGB Images\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 341/341 [00:35<00:00,  9.74it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Amount of augmented images 341\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "#---------- Augment Lettuce Dataset ----------\n",
                "depth_img_aug, rgb_imgs_aug, fresh_weight_GT, dry_weight_GT = data_augmentation.augment_data(rgb_images=rgb_list, depth_images=depth_list, fresh_weight_GT=fresh_weight_list, dry_weight_GT=dry_weight_list, amount_of_augmentated_images=augmented_dataset_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Size of images in the dataset:  (341, 4, 640, 480)  --> (Amount of images, total channels, img_w, img_h)\n",
                        "Size of images in the dataset:  (640, 480)\n",
                        "train, validation, test =  [255, 42, 44]\n"
                    ]
                }
            ],
            "source": [
                "#---------- Create data loaders ----------\n",
                "# Concatenate the depth and rgb images\n",
                "full_dataset = []\n",
                "full_dataset_labels = []\n",
                "for i in range(augmented_dataset_size):\n",
                "    depth_plus_rgb = np.concatenate((depth_img_aug[i], rgb_imgs_aug[i]), axis=0)\n",
                "    full_dataset.append(depth_plus_rgb)\n",
                "    #full_dataset.append([depth_img_aug[i], rgb_imgs_aug[i]])\n",
                "    full_dataset_labels.append([fresh_weight_GT[i], dry_weight_GT[i]])\n",
                "\n",
                "print(\"Size of images in the dataset: \", np.array(full_dataset).shape, \" --> (Amount of images, total channels, img_w, img_h)\")\n",
                "print(\"Size of images in the dataset: \", np.array(full_dataset[0][0]).shape)\n",
                "\n",
                "# Define the dataset\n",
                "dataset = lettuce_dataset.LettuceDataset(full_dataset, full_dataset_labels)\n",
                "\n",
                "# Split the dataset in train, validation and test\n",
                "splitted_data = lettuce_dataset.data_splittage(augmented_dataset_size, [75, 12.5, 12.5])\n",
                "print(\"train, validation, test = \", splitted_data)\n",
                "\n",
                "train_set, validation_set, test_set = torch.utils.data.random_split(dataset, splitted_data)\n",
                "#train_set, validation_set, test_set = torch.utils.data.random_split(dataset, [7500, 1250, 1250])\n",
                "\n",
                "# Create dataloaders for train, validation and test data\n",
                "train_loader = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
                "validation_loader = DataLoader(dataset = validation_set, batch_size = batch_size, shuffle = True)\n",
                "test_loader = DataLoader(dataset = test_set, batch_size = batch_size, shuffle = True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Model Definition ----------\n",
                "# Code modified from Anders Glent Buch elective course Deep Neural Networks\n",
                "# Tip: dont forget to include activation layer (ReLu)\n",
                "\n",
                "def build_cnn_layer(in_channels, out_channels, kernal_size=3, stride=1, padding=1, \n",
                "                    use_batchnorm=True, pool=False):\n",
                "    '''\n",
                "    in_channels: input channels to 2d convolution layer\n",
                "    out_channels: ouput channels to 2d convolution layer\n",
                "    kernal_size: kernal size (refer torch conv2d)\n",
                "    stride: stride (refer torch conv2d)\n",
                "    padding: padding (refer torch conv2d)  \n",
                "    use_batchnorm: enable/disable batchnorm (refer torch BatchNorm2d)\n",
                "    pool: enable/disable pooling (refer torch MaxPool2D)\n",
                "\n",
                "    should return the built CNN layer\n",
                "    '''\n",
                "    return torch.nn.Sequential(\n",
                "        torch.nn.Conv2d(in_channels, out_channels, kernal_size, stride=stride, padding=padding),\n",
                "        torch.nn.ReLU(inplace=True),\n",
                "        torch.nn.BatchNorm2d(out_channels) if use_batchnorm else torch.nn.Identity(),\n",
                "        torch.nn.MaxPool2d(4, 4) if pool else torch.nn.Identity(),\n",
                "    )\n",
                "\n",
                "# Remember to flatten after convolution layers before its passed to linear layers\n",
                "# Also activation layer for final layers\n",
                "def build_cnn_model():\n",
                "    '''\n",
                "    pool: enable/disable pooling\n",
                "\n",
                "    should return built CNN model\n",
                "    '''\n",
                "    return torch.nn.Sequential(\n",
                "        # (B, 4, 640, 480) images\n",
                "        build_cnn_layer(4, 8, stride=1, pool=True),\n",
                "        build_cnn_layer(8, 16, stride=1, pool=True), \n",
                "        build_cnn_layer(16, 32, stride=1, pool=True),\n",
                "        torch.nn.Flatten(),\n",
                "        torch.nn.Linear(2240, 1000),\n",
                "        torch.nn.ReLU(True),\n",
                "        torch.nn.Linear(1000, 512),\n",
                "        torch.nn.ReLU(True),\n",
                "        torch.nn.BatchNorm1d(512), # (B, 512)\n",
                "        torch.nn.Linear(512, 2), # (B, 2) logits\n",
                "    )    \n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Model declaration ----------\n",
                "model = build_cnn_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sequential(\n",
                        "  (0): Sequential(\n",
                        "    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
                        "    (1): ReLU(inplace=True)\n",
                        "    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
                        "  )\n",
                        "  (1): Sequential(\n",
                        "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
                        "    (1): ReLU(inplace=True)\n",
                        "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
                        "  )\n",
                        "  (2): Sequential(\n",
                        "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
                        "    (1): ReLU(inplace=True)\n",
                        "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
                        "  )\n",
                        "  (3): Flatten(start_dim=1, end_dim=-1)\n",
                        "  (4): Linear(in_features=2240, out_features=1000, bias=True)\n",
                        "  (5): ReLU(inplace=True)\n",
                        "  (6): Linear(in_features=1000, out_features=512, bias=True)\n",
                        "  (7): ReLU(inplace=True)\n",
                        "  (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "  (9): Linear(in_features=512, out_features=2, bias=True)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "print(model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cuda available =  True\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Auto select gpus: [0]\n",
                        "GPU available: True (cuda), used: True\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "HPU available: False, using: 0 HPUs\n",
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
                        "\n",
                        "  | Name      | Type                        | Params\n",
                        "----------------------------------------------------------\n",
                        "0 | model     | Sequential                  | 2.8 M \n",
                        "1 | loss_func | MeanAbsolutePercentageError | 0     \n",
                        "----------------------------------------------------------\n",
                        "2.8 M     Trainable params\n",
                        "0         Non-trainable params\n",
                        "2.8 M     Total params\n",
                        "11.047    Total estimated model params size (MB)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sanity Checking: 0it [00:00, ?it/s]"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
                        "  rank_zero_warn(\n",
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
                        "  rank_zero_warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                                                                           "
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
                        "  rank_zero_warn(\n",
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1555: PossibleUserWarning: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
                        "  rank_zero_warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5:  74%|███████▎  | 14/19 [00:08<00:03,  1.60it/s, loss=0.846, v_num=1]"
                    ]
                }
            ],
            "source": [
                "#---------- Training ----------\n",
                "print(\"Cuda available = \", torch.cuda.is_available())\n",
                "\n",
                "# Define the model with Pytorch Lightning\n",
                "model = model_and_training_files.BiomassModel(CNNmodel=model,\n",
                "                                                lr=learning_rate,\n",
                "                                                )\n",
                "\n",
                "trainer = model_and_training_files.get_trainer()    # gets the trainer (which is a class that takes the model and dataset)\n",
                "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=validation_loader) # Train the model\n",
                "trainer.test(model, dataloaders=test_loader)\n",
                "\n",
                "#---------- Save the weights ----------\n",
                "torch.save(model, \"saved_models/best_weights_V1.plk\") # Saves the regression head"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: 'saved_models/best_weights_V1.plk'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn [69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39msaved_models/best_weights_V1.plk\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m \u001b[39m#fresh_weight_GT, dry_weight_GT\u001b[39;00m\n\u001b[0;32m      3\u001b[0m imgidx \u001b[39m=\u001b[39m \u001b[39m6\u001b[39m\n",
                        "File \u001b[1;32mc:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
                        "File \u001b[1;32mc:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
                        "File \u001b[1;32mc:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
                        "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/best_weights_V1.plk'"
                    ]
                }
            ],
            "source": [
                "model = torch.load(\"saved_models/best_weights_V1.plk\")\n",
                "#fresh_weight_GT, dry_weight_GT\n",
                "imgidx = 6\n",
                "pred = model.prediction(depth_img_aug[imgidx], rgb_imgs_aug[imgidx])\n",
                "print(pred)\n",
                "print(fresh_weight_GT[imgidx], dry_weight_GT[imgidx])\n",
                "loss = MAPE()\n",
                "true = torch.from_numpy(np.array([[fresh_weight_GT[imgidx], dry_weight_GT[imgidx]]]))\n",
                "print(loss(pred,true))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.8 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "a569b293e0c6f5b8d5c7b027c77c352cc6b942ffb1eb577576f1f20f41e9121b"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
