{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The autoreload extension is already loaded. To reload it, use:\n",
                        "  %reload_ext autoreload\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "#---------- Library Imports ----------\n",
                "import torch\n",
                "import cv2\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "from torchvision import transforms\n",
                "import os\n",
                "import sequential_creator\n",
                "import glob\n",
                "from torchmetrics import MeanAbsolutePercentageError as MAPE\n",
                "import matplotlib as mpl\n",
                "import re\n",
                "import json\n",
                "from tqdm import tqdm\n",
                "import pytorch_lightning as pl\n",
                "from torch.utils.data import DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "%reload_ext autoreload"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Personal python files imports ----------\n",
                "import load_lettuce_dataset\n",
                "import data_augmentation\n",
                "import model_and_training_files\n",
                "import lettuce_dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Hyperparameters ----------\n",
                "batch_size = 16\n",
                "learning_rate = 1e-3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Other parameters ----------\n",
                "augmented_dataset_size = 50 # Size of the augmented dataset, so if the original dataset contained 103 images, they would be augmented and made into 2000 images\n",
                "                              # One thing to note is that if this number is much bigger than the size of the original dataset, then they would most likely end up being duplicates, since there is not that many augmentations implemented at the moment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 341/341 [02:10<00:00,  2.62it/s]\n"
                    ]
                }
            ],
            "source": [
                "#---------- Load Lettuce Dataset ----------\n",
                "rgb_list, depth_list, fresh_weight_list, dry_weight_list = load_lettuce_dataset.load_all_images()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Augmenting RGB Images\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 50/50 [01:05<00:00,  1.30s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Amount of augmented images 50\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "#---------- Augment Lettuce Dataset ----------\n",
                "depth_img_aug, rgb_imgs_aug, fresh_weight_GT, dry_weight_GT = data_augmentation.augment_data(rgb_images=rgb_list, depth_images=depth_list, fresh_weight_GT=fresh_weight_list, dry_weight_GT=dry_weight_list, amount_of_augmentated_images=augmented_dataset_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Size of images in the dataset:  (3, 224, 224)\n",
                        "train, validation, test =  [37, 6, 7]\n"
                    ]
                }
            ],
            "source": [
                "#---------- Create data loaders ----------\n",
                "# Concatenate the depth and rgb images\n",
                "full_dataset = []\n",
                "full_dataset_labels = []\n",
                "for i in range(augmented_dataset_size):\n",
                "    full_dataset.append([depth_img_aug[i], rgb_imgs_aug[i]])\n",
                "    full_dataset_labels.append([fresh_weight_GT[i], dry_weight_GT[i]])\n",
                "\n",
                "print(\"Size of images in the dataset: \", np.array(full_dataset[0][0]).shape)\n",
                "\n",
                "# Define the dataset\n",
                "dataset = lettuce_dataset.LettuceDataset(full_dataset, full_dataset_labels)\n",
                "\n",
                "# Split the dataset in train, validation and test\n",
                "splitted_data = lettuce_dataset.data_splittage(augmented_dataset_size, [75, 12.5, 12.5])\n",
                "print(\"train, validation, test = \", splitted_data)\n",
                "\n",
                "train_set, validation_set, test_set = torch.utils.data.random_split(dataset, splitted_data)\n",
                "#train_set, validation_set, test_set = torch.utils.data.random_split(dataset, [7500, 1250, 1250])\n",
                "\n",
                "# Create dataloaders for train, validation and test data\n",
                "train_loader = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
                "validation_loader = DataLoader(dataset = validation_set, batch_size = batch_size, shuffle = True)\n",
                "test_loader = DataLoader(dataset = test_set, batch_size = batch_size, shuffle = True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using cache found in C:\\Users\\Lasse/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sequential(\n",
                        "  (0): Linear(in_features=2000, out_features=1000, bias=True)\n",
                        "  (1): ReLU()\n",
                        "  (2): Linear(in_features=1000, out_features=500, bias=True)\n",
                        "  (3): ReLU()\n",
                        "  (4): Linear(in_features=500, out_features=250, bias=True)\n",
                        "  (5): ReLU()\n",
                        "  (6): Linear(in_features=250, out_features=2, bias=True)\n",
                        "  (7): ReLU()\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "#---------- Model Definition ----------\n",
                "# Load ResNet Backbone (Pretrained Model)\n",
                "resnet_version = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']\n",
                "# We can reuse the same resnet model since weights are not updated and both RGB and Depth are same format\n",
                "resnet_model_RGB_and_depth = torch.hub.load('pytorch/vision:v0.10.0', resnet_version[3], weights=\"ResNet101_Weights.DEFAULT\")\n",
                "resnet_model_RGB_and_depth.eval() # We don't want the resnet to update (Sets a flag, kind of a switch to turn off gradient computation)\n",
                "\n",
                "# Create Regression Head\n",
                "head_input_neurons = 2000 # 1000 output feature vector from RGB resnet and 1000 from depth resnet\n",
                "head_hidden = [1000, 500, 250]\n",
                "head_output_neurons = 2\n",
                "head_activation = torch.nn.ReLU()\n",
                "regression_head = sequential_creator.make_model(input=head_input_neurons, hidden=head_hidden, output=head_output_neurons, activation=head_activation)\n",
                "\n",
                "print(regression_head)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "GPU available: False, used: False\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "HPU available: False, using: 0 HPUs\n",
                        "\n",
                        "  | Name   | Type       | Params\n",
                        "--------------------------------------\n",
                        "0 | layers | Sequential | 2.6 M \n",
                        "1 | Resnet | ResNet     | 44.5 M\n",
                        "--------------------------------------\n",
                        "47.2 M    Trainable params\n",
                        "0         Non-trainable params\n",
                        "47.2 M    Total params\n",
                        "188.706   Total estimated model params size (MB)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "False\n",
                        "Epoch 0: 100%|██████████| 4/4 [00:26<00:00,  6.67s/it, loss=1.15, v_num=2] "
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 0: 100%|██████████| 4/4 [00:31<00:00,  8.00s/it, loss=1.15, v_num=2]\n",
                        "Testing DataLoader 0: 100%|██████████| 1/1 [00:03<00:00,  3.98s/it]\n",
                        "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
                        "       Test metric             DataLoader 0\n",
                        "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
                        "        test_loss           0.8784412145614624\n",
                        "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
                    ]
                }
            ],
            "source": [
                "%reload_ext autoreload\n",
                "#---------- Training ----------\n",
                "print(torch.cuda.is_available())\n",
                "\n",
                "# Define the model with Pytorch Lightning\n",
                "model = model_and_training_files.BiomassModel(regression_head=regression_head, \n",
                "                                                resnet_model_RGB_and_depth=resnet_model_RGB_and_depth,\n",
                "                                                lr=learning_rate,\n",
                "                                                )\n",
                "\n",
                "trainer = model_and_training_files.get_trainer(max_epochs=1)    # gets the trainer (which is a class that takes the model and dataset)\n",
                "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=validation_loader) # Train the model\n",
                "trainer.test(model, dataloaders=test_loader)\n",
                "\n",
                "#---------- Save the weights ----------\n",
                "torch.save(model, \"saved_models/best_weights_V1.plk\") # Saves the regression head"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.0 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "3872e51830101b1d178418ab45f15963383a34a7de805094578585ef84da2345"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
