{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "#---------- Library Imports ----------\n",
                "import torch\n",
                "import cv2\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "from torchvision import transforms\n",
                "import os\n",
                "import sequential_creator\n",
                "import glob\n",
                "from torchmetrics import MeanAbsolutePercentageError as MAPE\n",
                "import matplotlib as mpl\n",
                "import re\n",
                "import json\n",
                "from tqdm import tqdm\n",
                "import pytorch_lightning as pl\n",
                "from torch.utils.data import DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "%reload_ext autoreload"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Personal python files imports ----------\n",
                "import load_lettuce_dataset\n",
                "import data_augmentation\n",
                "import model_and_training_files\n",
                "import lettuce_dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Hyperparameters ----------\n",
                "batch_size = 16\n",
                "learning_rate = 1e-3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Other parameters ----------\n",
                "augmented_dataset_size = 320 # Size of the augmented dataset, so if the original dataset contained 103 images, they would be augmented and made into 2000 images\n",
                "                              # One thing to note is that if this number is much bigger than the size of the original dataset, then they would most likely end up being duplicates, since there is not that many augmentations implemented at the moment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 341/341 [00:01<00:00, 318.40it/s]\n"
                    ]
                }
            ],
            "source": [
                "#---------- Load Lettuce Dataset ----------\n",
                "rgb_list, depth_list, fresh_weight_list, dry_weight_list = load_lettuce_dataset.load_all_images()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Augmenting RGB Images\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 320/320 [00:32<00:00,  9.89it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Amount of augmented images 320\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "#---------- Augment Lettuce Dataset ----------\n",
                "depth_img_aug, rgb_imgs_aug, fresh_weight_GT, dry_weight_GT = data_augmentation.augment_data(rgb_images=rgb_list, depth_images=depth_list, fresh_weight_GT=fresh_weight_list, dry_weight_GT=dry_weight_list, amount_of_augmentated_images=augmented_dataset_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Size of images in the dataset:  (320, 4, 640, 480)  --> (Amount of images, total channels, img_w, img_h)\n",
                        "Size of images in the dataset:  (640, 480)\n",
                        "train, validation, test =  [240, 40, 40]\n"
                    ]
                }
            ],
            "source": [
                "#---------- Create data loaders ----------\n",
                "# Concatenate the depth and rgb images\n",
                "full_dataset = []\n",
                "full_dataset_labels = []\n",
                "for i in range(augmented_dataset_size):\n",
                "    depth_plus_rgb = np.concatenate((depth_img_aug[i], rgb_imgs_aug[i]), axis=0)\n",
                "    full_dataset.append(depth_plus_rgb)\n",
                "    #full_dataset.append([depth_img_aug[i], rgb_imgs_aug[i]])\n",
                "    #full_dataset_labels.append([fresh_weight_GT[i], dry_weight_GT[i]])\n",
                "    full_dataset_labels.append([dry_weight_GT[i]])\n",
                "\n",
                "print(\"Size of images in the dataset: \", np.array(full_dataset).shape, \" --> (Amount of images, total channels, img_w, img_h)\")\n",
                "print(\"Size of images in the dataset: \", np.array(full_dataset[0][0]).shape)\n",
                "\n",
                "# Define the dataset\n",
                "dataset = lettuce_dataset.LettuceDataset(full_dataset, full_dataset_labels)\n",
                "\n",
                "# Split the dataset in train, validation and test\n",
                "splitted_data = lettuce_dataset.data_splittage(augmented_dataset_size, [75, 12.5, 12.5])\n",
                "print(\"train, validation, test = \", splitted_data)\n",
                "\n",
                "train_set, validation_set, test_set = torch.utils.data.random_split(dataset, splitted_data)\n",
                "#train_set, validation_set, test_set = torch.utils.data.random_split(dataset, [7500, 1250, 1250])\n",
                "\n",
                "# Create dataloaders for train, validation and test data\n",
                "train_loader = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
                "validation_loader = DataLoader(dataset = validation_set, batch_size = batch_size, shuffle = True)\n",
                "test_loader = DataLoader(dataset = test_set, batch_size = batch_size, shuffle = True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Model Definition ----------\n",
                "# Code modified from Anders Glent Buch elective course Deep Neural Networks\n",
                "# Tip: dont forget to include activation layer (ReLu)\n",
                "\n",
                "def build_cnn_layer(in_channels, out_channels, kernal_size=3, stride=1, padding=1, \n",
                "                    use_batchnorm=True, pool=False):\n",
                "    '''\n",
                "    in_channels: input channels to 2d convolution layer\n",
                "    out_channels: ouput channels to 2d convolution layer\n",
                "    kernal_size: kernal size (refer torch conv2d)\n",
                "    stride: stride (refer torch conv2d)\n",
                "    padding: padding (refer torch conv2d)  \n",
                "    use_batchnorm: enable/disable batchnorm (refer torch BatchNorm2d)\n",
                "    pool: enable/disable pooling (refer torch MaxPool2D)\n",
                "\n",
                "    should return the built CNN layer\n",
                "    '''\n",
                "    return torch.nn.Sequential(\n",
                "        torch.nn.Conv2d(in_channels, out_channels, kernal_size, stride=stride, padding=padding),\n",
                "        torch.nn.ReLU(inplace=True),\n",
                "        torch.nn.BatchNorm2d(out_channels) if use_batchnorm else torch.nn.Identity(),\n",
                "        torch.nn.MaxPool2d(3, 3) if pool else torch.nn.Identity(),\n",
                "    )\n",
                "\n",
                "# Remember to flatten after convolution layers before its passed to linear layers\n",
                "# Also activation layer for final layers\n",
                "def build_cnn_model():\n",
                "    '''\n",
                "    pool: enable/disable pooling\n",
                "\n",
                "    should return built CNN model\n",
                "    '''\n",
                "    return torch.nn.Sequential(\n",
                "        # (B, 4, 640, 480) images\n",
                "        build_cnn_layer(4, 8, stride=1, pool=True),\n",
                "        build_cnn_layer(8, 16, stride=1, pool=True), \n",
                "        build_cnn_layer(16, 32, stride=1, pool=True),\n",
                "        torch.nn.Flatten(),\n",
                "        torch.nn.Linear(12512, 1000),\n",
                "        torch.nn.ReLU(True),\n",
                "        torch.nn.Linear(1000, 512),\n",
                "        torch.nn.ReLU(True),\n",
                "        torch.nn.BatchNorm1d(512), # (B, 512)\n",
                "        torch.nn.Linear(512, 1), # (B, 2) logits\n",
                "    )    \n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "#---------- Model declaration ----------\n",
                "model = build_cnn_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sequential(\n",
                        "  (0): Sequential(\n",
                        "    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
                        "    (1): ReLU(inplace=True)\n",
                        "    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
                        "  )\n",
                        "  (1): Sequential(\n",
                        "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
                        "    (1): ReLU(inplace=True)\n",
                        "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
                        "  )\n",
                        "  (2): Sequential(\n",
                        "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
                        "    (1): ReLU(inplace=True)\n",
                        "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
                        "  )\n",
                        "  (3): Flatten(start_dim=1, end_dim=-1)\n",
                        "  (4): Linear(in_features=12512, out_features=1000, bias=True)\n",
                        "  (5): ReLU(inplace=True)\n",
                        "  (6): Linear(in_features=1000, out_features=512, bias=True)\n",
                        "  (7): ReLU(inplace=True)\n",
                        "  (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "  (9): Linear(in_features=512, out_features=1, bias=True)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "print(model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cuda available =  True\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Auto select gpus: [0]\n",
                        "GPU available: True (cuda), used: True\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "HPU available: False, using: 0 HPUs\n",
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
                        "\n",
                        "  | Name      | Type       | Params\n",
                        "-----------------------------------------\n",
                        "0 | model     | Sequential | 13.0 M\n",
                        "1 | loss_func | MSELoss    | 0     \n",
                        "-----------------------------------------\n",
                        "13.0 M    Trainable params\n",
                        "0         Non-trainable params\n",
                        "13.0 M    Total params\n",
                        "52.133    Total estimated model params size (MB)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sanity Checking: 0it [00:00, ?it/s]"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
                        "  rank_zero_warn(\n",
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
                        "  rank_zero_warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                                                                           "
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
                        "  rank_zero_warn(\n",
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1555: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
                        "  rank_zero_warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 49: 100%|██████████| 18/18 [00:06<00:00,  2.83it/s, loss=1.09, v_num=6] "
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 49: 100%|██████████| 18/18 [00:07<00:00,  2.53it/s, loss=1.09, v_num=6]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:488: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
                        "  rank_zero_warn(\n",
                        "c:\\Users\\Ulric\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
                        "  rank_zero_warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Testing DataLoader 0: 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]\n",
                        "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
                        "       Test metric             DataLoader 0\n",
                        "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
                        "        test_loss           15.445657730102539\n",
                        "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[{'test_loss': 15.445657730102539}]"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#---------- Training ----------\n",
                "print(\"Cuda available = \", torch.cuda.is_available())\n",
                "\n",
                "# Define the model with Pytorch Lightning\n",
                "model = model_and_training_files.BiomassModel(CNNmodel=model,\n",
                "                                                lr=learning_rate,\n",
                "                                                )\n",
                "\n",
                "trainer = model_and_training_files.get_trainer()    # gets the trainer (which is a class that takes the model and dataset)\n",
                "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=validation_loader) # Train the model\n",
                "trainer.test(model, dataloaders=test_loader)\n",
                "\n",
                "#---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "#------- Save the weights ----------\n",
                "#torch.save(model, \"saved_models/best_weights_V1.plk\") # Saves the regression head"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[8.1313]])\n",
                        "[8.77]\n"
                    ]
                }
            ],
            "source": [
                "#model = torch.load(\"saved_models/best_weights_V1.plk\")\n",
                "#fresh_weight_GT, dry_weight_GT\n",
                "imgidx = 275\n",
                "#print(full_dataset[imgidx].shape)\n",
                "#print(torch.tensor(full_dataset[imgidx]).float().shape)\n",
                "model.eval()\n",
                "pred = model.prediction(torch.tensor(np.array(dataset.data[imgidx])).float())\n",
                "print(pred)\n",
                "print(full_dataset_labels[imgidx])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.8 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "a569b293e0c6f5b8d5c7b027c77c352cc6b942ffb1eb577576f1f20f41e9121b"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
